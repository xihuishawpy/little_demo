{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle \n",
    "from paddle.nn import Linear\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    load_boston = datasets.load_boston()\n",
    "    X = pd.DataFrame(load_boston['data'],columns=load_boston['feature_names'])\n",
    "    y = pd.DataFrame(load_boston['target'],columns=['target'])\n",
    "    data = pd.concat([X,y],axis=1).values\n",
    "    feature_num = len(load_boston['feature_names']) \n",
    "    \n",
    "    # 划分训练集\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0]*ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 归一化\n",
    "    maximums, minimums = training_data.max(axis=0), training_data.min(axis=0)\n",
    "    \n",
    "    # 全局变量 记录数据的归一化参数，在预测时对数据做归一化\n",
    "    # 注意：只拿训练集的归一化参数进行归一化，不然会泄露到测试集\n",
    "    global max_values\n",
    "    global min_values\n",
    "   \n",
    "    max_values = maximums\n",
    "    min_values = minimums\n",
    "    \n",
    "    # 对数据进行归一化\n",
    "    for i in range(feature_num):\n",
    "        data[:,i] = (data[:,i]- min_values[i]) / (max_values[i]- min_values[i])\n",
    "        \n",
    "    \n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data , test_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 14)\n",
      "[2.35922539e-04 0.00000000e+00 2.62405717e-01 0.00000000e+00\n",
      " 1.72839506e-01 5.47997701e-01 7.82698249e-01 3.48961980e-01\n",
      " 4.34782609e-02 1.14822547e-01 5.53191489e-01 1.00000000e+00\n",
      " 2.04470199e-01 2.16000000e+01]\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data = load_data()\n",
    "print(training_data.shape)\n",
    "print(training_data[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(paddle.nn.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 初始化 父类参数\n",
    "        super(Regressor,self).__init__()\n",
    "        # 定义网络，全连接\n",
    "        self.fc = Linear(in_features=13,out_features=1)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        x = self.fc(input)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化类实例\n",
    "model = Regressor()\n",
    "# 开启训练\n",
    "model.train()\n",
    "#定义优化算法\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.01,parameters=model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [50.793915]\n",
      "epoch: 0, iter: 20, loss is: [55.211212]\n",
      "epoch: 0, iter: 40, loss is: [200.32037]\n",
      "epoch: 1, iter: 0, loss is: [125.28409]\n",
      "epoch: 1, iter: 20, loss is: [153.13754]\n",
      "epoch: 1, iter: 40, loss is: [86.842285]\n",
      "epoch: 2, iter: 0, loss is: [95.10344]\n",
      "epoch: 2, iter: 20, loss is: [62.061363]\n",
      "epoch: 2, iter: 40, loss is: [19.123714]\n",
      "epoch: 3, iter: 0, loss is: [207.5177]\n",
      "epoch: 3, iter: 20, loss is: [135.27391]\n",
      "epoch: 3, iter: 40, loss is: [114.60943]\n",
      "epoch: 4, iter: 0, loss is: [30.73014]\n",
      "epoch: 4, iter: 20, loss is: [87.01141]\n",
      "epoch: 4, iter: 40, loss is: [39.162884]\n",
      "epoch: 5, iter: 0, loss is: [142.39726]\n",
      "epoch: 5, iter: 20, loss is: [52.89307]\n",
      "epoch: 5, iter: 40, loss is: [183.2455]\n",
      "epoch: 6, iter: 0, loss is: [32.101078]\n",
      "epoch: 6, iter: 20, loss is: [89.32934]\n",
      "epoch: 6, iter: 40, loss is: [57.3537]\n",
      "epoch: 7, iter: 0, loss is: [32.868263]\n",
      "epoch: 7, iter: 20, loss is: [157.14088]\n",
      "epoch: 7, iter: 40, loss is: [174.61716]\n",
      "epoch: 8, iter: 0, loss is: [131.56956]\n",
      "epoch: 8, iter: 20, loss is: [124.72713]\n",
      "epoch: 8, iter: 40, loss is: [44.824875]\n",
      "epoch: 9, iter: 0, loss is: [155.46982]\n",
      "epoch: 9, iter: 20, loss is: [105.01163]\n",
      "epoch: 9, iter: 40, loss is: [37.031704]\n"
     ]
    }
   ],
   "source": [
    "# 设置外层循环次数、batch大小\n",
    "EPOCH_NUM = 10   \n",
    "BATCH_SIZE = 10  \n",
    "\n",
    "# 分10个epoch，每个epoch按10个样本为一个batch\n",
    "for epoch_id in range(EPOCH_NUM):\n",
    "    # 训练数据随机打散\n",
    "    np.random.shuffle(training_data)\n",
    "    mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0,len(training_data),BATCH_SIZE)]\n",
    "    for iter_id , mini_batch in enumerate(mini_batches):\n",
    "        x = np.array(mini_batch[:, :-1])\n",
    "        y = np.array(mini_batch[:,-1])\n",
    "        \n",
    "        # 转成tensor \n",
    "        # 天坑：不指定float32就报错，不懂~\n",
    "        house_features = paddle.to_tensor(x,dtype='float32')\n",
    "        prices = paddle.to_tensor(y,dtype='float32')\n",
    "        \n",
    "        # 前向传播\n",
    "        predicts = model.forward(house_features)\n",
    "        \n",
    "        #计算损失\n",
    "        loss = F.square_error_cost(predicts,label=prices)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        # 每 20个样本打印一次loss\n",
    "        if iter_id % 20 == 0 :\n",
    "            print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\n",
    "        \n",
    "        # 反向传播 计算梯度\n",
    "        avg_loss.backward()\n",
    "        # 更新参数\n",
    "        opt.step()\n",
    "        # 清除梯度\n",
    "        opt.clear_grad()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型保存 ,用于后续预测、调用\n",
    "paddle.save(model.state_dict(),'LR_model.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result is [[21.835506]\n",
      " [23.405983]\n",
      " [21.526829]], '\n",
      "' the corresponding label is Tensor(shape=[3], dtype=float32, place=Place(cpu), stop_gradient=True,\n",
      "       [8.50000000 , 5.         , 11.89999962])\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "\n",
    "model_dict = paddle.load('LR_model.pdparams')\n",
    "model.load_dict(model_dict)\n",
    "# 模型调整为预测（校验）模式\n",
    "model.eval()\n",
    "\n",
    "\n",
    "test_x = paddle.to_tensor(test_data[:3, :-1],dtype='float32')\n",
    "test_y = paddle.to_tensor(test_data[:3, -1],dtype='float32')\n",
    "\n",
    "# 预测\n",
    "predict = model.forward(test_x)\n",
    "\n",
    "# # 对结果做反归一化处理\n",
    "# predict = predict * (max_values[-1] - min_values[-1]) + min_values[-1]\n",
    "# # 对label数据做反归一化处理\n",
    "# test_y = test_y * (max_values[-1] - min_values[-1]) + min_values[-1]\n",
    "\n",
    "print(\"Inference result is {}, '\\n' the corresponding label is {}\".format(predict.numpy(), test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('paddle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc03531e3015553affe1fa0734fdd021a885eb8a88bb99dabf4a3c95a410d7e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
