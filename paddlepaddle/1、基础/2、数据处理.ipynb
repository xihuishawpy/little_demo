{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![20221124165432](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20221124165432.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Linear\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MINIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MINIST, self).__init__()\n",
    "        self.fc = paddle.nn.Linear(784, 1)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return self.fc(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载json数据 \n",
    "data = json.load(gzip.open('mnist.json.gz'))\n",
    "train_set, val_set, eval_set = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取与模型训练串行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(mode='train'):\n",
    "    # 加载json数据\n",
    "    data = json.load(gzip.open('mnist.json.gz'))\n",
    "   \n",
    "    # 数据读取\n",
    "    train_set, val_set, eval_set = data\n",
    "    if mode=='train':\n",
    "        imgs, labels = train_set[0], train_set[1]\n",
    "    elif mode=='valid':\n",
    "        imgs, labels = val_set[0], val_set[1]\n",
    "    elif mode=='eval':\n",
    "        imgs, labels = eval_set[0], eval_set[1]\n",
    "    else:\n",
    "        raise Exception(\"只能有这3种读取模式：['train', 'valid', 'eval']\")\n",
    "    print(\"数据集数量: \", len(imgs))\n",
    "    \n",
    "    # 校验数据\n",
    "    imgs_length = len(imgs)\n",
    "\n",
    "    assert len(imgs) == len(labels), \\\n",
    "          \"样本长度({})=label长度({})\".format(len(imgs), len(labels))\n",
    "    \n",
    "    # 定义数据集每个数据的序号，根据序号读取数据\n",
    "    index_list = list(range(imgs_length))\n",
    "    BATCHSIZE = 100\n",
    "    \n",
    "    # 数据生成器\n",
    "    def data_generator():\n",
    "        if mode == 'train':\n",
    "            np.random.shuffle(index_list)\n",
    "        imgs_list = []\n",
    "        labels_list = []\n",
    "        for i in index_list:\n",
    "            img = np.array(imgs[i]).astype('float32')\n",
    "            label = np.array(labels[i]).astype('float32')\n",
    "            imgs_list.append(img) \n",
    "            labels_list.append(label)\n",
    "            if len(imgs_list) == BATCHSIZE:\n",
    "                # 获得一个batchsize数据，并返回\n",
    "                # 生成模式，减少内存占用\n",
    "                yield np.array(imgs_list), np.array(labels_list)\n",
    "                # 清空数据读取列表\n",
    "                imgs_list = []\n",
    "                labels_list = []\n",
    "    \n",
    "        # 剩余小于一个batch的数据，单独组成一个batch\n",
    "        if len(imgs_list) > 0:\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集数量:  50000\n",
      "epoch: 0, batch: 0, loss is: [24.82596]\n",
      "epoch: 0, batch: 100, loss is: [8.885684]\n",
      "epoch: 0, batch: 200, loss is: [8.884466]\n",
      "epoch: 0, batch: 300, loss is: [10.054378]\n",
      "epoch: 0, batch: 400, loss is: [10.07171]\n",
      "epoch: 1, batch: 0, loss is: [7.0039916]\n",
      "epoch: 1, batch: 100, loss is: [9.610771]\n",
      "epoch: 1, batch: 200, loss is: [9.0718]\n",
      "epoch: 1, batch: 300, loss is: [9.122907]\n",
      "epoch: 1, batch: 400, loss is: [9.951382]\n",
      "epoch: 2, batch: 0, loss is: [9.5425205]\n",
      "epoch: 2, batch: 100, loss is: [9.823632]\n",
      "epoch: 2, batch: 200, loss is: [9.732441]\n",
      "epoch: 2, batch: 300, loss is: [8.533162]\n",
      "epoch: 2, batch: 400, loss is: [9.047721]\n",
      "epoch: 3, batch: 0, loss is: [9.799807]\n",
      "epoch: 3, batch: 100, loss is: [8.877254]\n",
      "epoch: 3, batch: 200, loss is: [8.487012]\n",
      "epoch: 3, batch: 300, loss is: [8.902817]\n",
      "epoch: 3, batch: 400, loss is: [8.861171]\n",
      "epoch: 4, batch: 0, loss is: [9.992799]\n",
      "epoch: 4, batch: 100, loss is: [9.144516]\n",
      "epoch: 4, batch: 200, loss is: [8.174755]\n",
      "epoch: 4, batch: 300, loss is: [9.321756]\n",
      "epoch: 4, batch: 400, loss is: [8.37832]\n",
      "epoch: 5, batch: 0, loss is: [8.160917]\n",
      "epoch: 5, batch: 100, loss is: [7.7814455]\n",
      "epoch: 5, batch: 200, loss is: [7.9534335]\n",
      "epoch: 5, batch: 300, loss is: [8.171667]\n",
      "epoch: 5, batch: 400, loss is: [9.563227]\n",
      "epoch: 6, batch: 0, loss is: [9.255438]\n",
      "epoch: 6, batch: 100, loss is: [9.281012]\n",
      "epoch: 6, batch: 200, loss is: [8.547353]\n",
      "epoch: 6, batch: 300, loss is: [9.241725]\n",
      "epoch: 6, batch: 400, loss is: [9.076732]\n",
      "epoch: 7, batch: 0, loss is: [8.569954]\n",
      "epoch: 7, batch: 100, loss is: [8.2783785]\n",
      "epoch: 7, batch: 200, loss is: [7.956006]\n",
      "epoch: 7, batch: 300, loss is: [9.464204]\n",
      "epoch: 7, batch: 400, loss is: [8.595619]\n",
      "epoch: 8, batch: 0, loss is: [8.935213]\n",
      "epoch: 8, batch: 100, loss is: [8.341566]\n",
      "epoch: 8, batch: 200, loss is: [8.299154]\n",
      "epoch: 8, batch: 300, loss is: [7.962608]\n",
      "epoch: 8, batch: 400, loss is: [8.609027]\n",
      "epoch: 9, batch: 0, loss is: [9.16797]\n",
      "epoch: 9, batch: 100, loss is: [7.5321445]\n",
      "epoch: 9, batch: 200, loss is: [8.663883]\n",
      "epoch: 9, batch: 300, loss is: [8.810266]\n",
      "epoch: 9, batch: 400, loss is: [9.585724]\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # 实例化网络，开启训练\n",
    "    model = MINIST()\n",
    "    model.train()\n",
    "    \n",
    "    # 数据读取\n",
    "    train_loader = load_data(mode='train')\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n",
    "    \n",
    "    EPOCH = 10 \n",
    "    for epoch_id in range(EPOCH):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            images ,  labels = data\n",
    "            images = paddle.to_tensor(images,dtype='float32')\n",
    "            labels = paddle.to_tensor(labels,dtype='float32')\n",
    "            \n",
    "            # 正向传播\n",
    "            predicts = model.forward(images)\n",
    "            loss = F.square_error_cost(predicts,label=labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n",
    "            \n",
    "            # 反向传播，更新参数\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        \n",
    "    paddle.save(model.state_dict(),'mnist.pdparams')\n",
    "\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![20221125101603](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20221125101603.png)\n",
    "\n",
    "## 异步数据读取\n",
    "\n",
    "数据读取和模型训练**并行**。读取到的数据不断的放入缓存区，无需等待模型训练就可以启动下一轮数据读取。当模型训练完一个批次后，不用等待数据读取过程，直接从缓存区获得下一批次数据进行训练，从而加快了数据读取速度。\n",
    "\n",
    "异步队列：数据读取和模型训练交互的仓库，二者均可以从仓库中读取数据，它的存在使得两者的工作节奏可以解耦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 继承paddle.io.Dataset类的数据读取器\n",
    "class MnistDataset(paddle.io.Dataset):\n",
    "\n",
    "    # 实现初始化方法，在初始化的时候将数据读载入\n",
    "    def __init__(self,mode):\n",
    "        data = json.load(gzip.open('mnist.json.gz'))\n",
    "        # 数据读取\n",
    "        train_set, val_set, eval_set = data\n",
    "        if mode=='train':\n",
    "            imgs, labels = train_set[0], train_set[1]\n",
    "        elif mode=='valid':\n",
    "            imgs, labels = val_set[0], val_set[1]\n",
    "        elif mode=='eval':\n",
    "            imgs, labels = eval_set[0], eval_set[1]\n",
    "        else:\n",
    "            raise Exception(\"只能有这3种读取模式：['train', 'valid', 'eval']\")\n",
    "        print(\"数据集数量: \", len(imgs))\n",
    "        \n",
    "        # 校验数据\n",
    "        imgs_length = len(imgs)\n",
    "        assert len(imgs) == len(labels), \\\n",
    "            \"样本长度({})=label长度({})\".format(len(imgs), len(labels))\n",
    "            \n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "\n",
    "    # __getitem__() 该方法定义用索引(0 到 len(self))获取一条数据或一个样本\n",
    "    def __getitem__(self,idx):\n",
    "        img = np.array(self.imgs[idx]).astype('float32')\n",
    "        label = np.array(self.labels[idx]).astype('float32')\n",
    "        return img ,label\n",
    "     \n",
    "    # 返回数据集的总长度\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集数量:  50000\n"
     ]
    }
   ],
   "source": [
    "# 实例化一个对象访问 定义好的数据集\n",
    "train_dataset = MnistDataset(mode='train')\n",
    "\n",
    "# 用paddle.io.DataLoader 定义DataLoader对象用于加载Python生成器产生的数据\n",
    "# DataLoader 返回的是一个批次数据迭代器，并且是异步的\n",
    "data_loader = paddle.io.DataLoader(train_dataset , batch_size=100, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch id = 0, batch id = 0, loss = 31.080965\n",
      "epoch id = 0, batch id = 100, loss = 11.579112\n",
      "epoch id = 0, batch id = 200, loss = 9.516181\n",
      "epoch id = 0, batch id = 300, loss = 8.596170\n",
      "epoch id = 0, batch id = 400, loss = 10.533883\n",
      "epoch id = 1, batch id = 0, loss = 10.032254\n",
      "epoch id = 1, batch id = 100, loss = 9.104719\n",
      "epoch id = 1, batch id = 200, loss = 9.645576\n",
      "epoch id = 1, batch id = 300, loss = 8.424115\n",
      "epoch id = 1, batch id = 400, loss = 10.812142\n",
      "epoch id = 2, batch id = 0, loss = 8.437902\n",
      "epoch id = 2, batch id = 100, loss = 9.696856\n",
      "epoch id = 2, batch id = 200, loss = 9.246710\n",
      "epoch id = 2, batch id = 300, loss = 8.111835\n",
      "epoch id = 2, batch id = 400, loss = 9.740843\n",
      "epoch id = 3, batch id = 0, loss = 9.318897\n",
      "epoch id = 3, batch id = 100, loss = 9.657000\n",
      "epoch id = 3, batch id = 200, loss = 8.939503\n",
      "epoch id = 3, batch id = 300, loss = 10.104265\n",
      "epoch id = 3, batch id = 400, loss = 9.695880\n",
      "epoch id = 4, batch id = 0, loss = 8.546838\n",
      "epoch id = 4, batch id = 100, loss = 9.141880\n",
      "epoch id = 4, batch id = 200, loss = 7.935027\n",
      "epoch id = 4, batch id = 300, loss = 8.936279\n",
      "epoch id = 4, batch id = 400, loss = 9.120320\n",
      "epoch id = 5, batch id = 0, loss = 9.101582\n",
      "epoch id = 5, batch id = 100, loss = 9.608073\n",
      "epoch id = 5, batch id = 200, loss = 9.235993\n",
      "epoch id = 5, batch id = 300, loss = 8.836065\n",
      "epoch id = 5, batch id = 400, loss = 8.610616\n",
      "epoch id = 6, batch id = 0, loss = 8.629976\n",
      "epoch id = 6, batch id = 100, loss = 10.092611\n",
      "epoch id = 6, batch id = 200, loss = 8.188954\n",
      "epoch id = 6, batch id = 300, loss = 8.610961\n",
      "epoch id = 6, batch id = 400, loss = 7.262867\n",
      "epoch id = 7, batch id = 0, loss = 8.985538\n",
      "epoch id = 7, batch id = 100, loss = 9.173461\n",
      "epoch id = 7, batch id = 200, loss = 9.964911\n",
      "epoch id = 7, batch id = 300, loss = 9.019663\n",
      "epoch id = 7, batch id = 400, loss = 7.739170\n",
      "epoch id = 8, batch id = 0, loss = 9.327946\n",
      "epoch id = 8, batch id = 100, loss = 8.081934\n",
      "epoch id = 8, batch id = 200, loss = 8.548770\n",
      "epoch id = 8, batch id = 300, loss = 8.316082\n",
      "epoch id = 8, batch id = 400, loss = 8.502028\n",
      "epoch id = 9, batch id = 0, loss = 7.707688\n",
      "epoch id = 9, batch id = 100, loss = 8.131807\n",
      "epoch id = 9, batch id = 200, loss = 8.476919\n",
      "epoch id = 9, batch id = 300, loss = 8.269551\n",
      "epoch id = 9, batch id = 400, loss = 7.489801\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model = MINIST()\n",
    "    model.train\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n",
    "\n",
    "    EPOCH = 10\n",
    "    for epoch_id in range(EPOCH):\n",
    "        for batch_id, data in enumerate(data_loader()):     \n",
    "            images , labels = data\n",
    "            images = paddle.to_tensor(images,dtype ='float32')\n",
    "            labels = paddle.to_tensor(labels, dtype='float32') \n",
    "            \n",
    "            predicts = model(images)\n",
    "            loss = F.square_error_cost(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch id = %d, batch id = %d, loss = %f\" % (epoch_id, batch_id, avg_loss.numpy()))\n",
    "            \n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "            \n",
    "    paddle.save(model.state_dict() , 'minist.pdparams')\n",
    "    \n",
    "                        \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc03531e3015553affe1fa0734fdd021a885eb8a88bb99dabf4a3c95a410d7e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}