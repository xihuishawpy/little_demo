{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成算法的目的是将几个基估计器的预测与给定的学习算法结合起来，以提高单个估计器的通用性和鲁棒性。\n",
    "\n",
    "集成方法一般分为两种：\n",
    "\n",
    "- 在平均法（averaging methods)中，该方法的原理是构建多个独立的估计器，然后`取预测结果的平均`。一般来说，组合之后的估计器是会比单个估计器要好，因为`方差减小`了。\n",
    "\n",
    "   - 示例: Bagging methods, Forests of randomized trees, …\n",
    "\n",
    "- 相反，在提升法(boosting methods)中，`基估计器是按顺序建立的，降低了偏差`。其动机是将几个弱模型结合起来，形成一个强大的整体。\n",
    "\n",
    "    - 示例: AdaBoost, Gradient Tree Boosting, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "在集成算法中， bagging方法形成了一类算法，它在原始训练集的随机子集上建立几个黑箱估计器的实例，然后将它们的个体预测聚合起来，形成最终的预测。\n",
    "\n",
    "这些方法通过在基本估计器(例如决策树)的构造过程中`引入随机化`，然后`将其集成起来`，从而`降低单个基本估计器(如决策树)的方差`。\n",
    "\n",
    "在许多情况下，bagging方法是一个非常简单的方法可以用来改进相对单一模型，而不需要调整底层。由于bagging方法提供了一种减少过度拟合的途径，因此**对强大模型和复杂模型(例如，充分生长的决策树)最有效**，与之对比的提升法在弱模型(例如浅层决策树)上表现最好。\n",
    "\n",
    "![20220712152323](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220712152323.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林 -- RandomForestClassifier\n",
    "\n",
    "在随机森林中(RandomForestClassifier和RandomForestRegressor类)， 集成模型中的每棵树构建时的样本都是由`训练集经过有放回抽样`(比如a bootstrap sample自助式采样法)得来。\n",
    "\n",
    "在构建树的过程中进行结点分割时，选择的分割点是所有特征的最佳分割点，或特征的大小为 max_features 的随机子集的最佳分割。\n",
    "\n",
    "这两种随机的目的是`降低森林估计器的方差`。事实上，单个决策树通常表现出很高的方差，并且往往会过拟合。在森林中注入随机性产生的决策树具有一定的解耦预测误差(decoupled prediction errors)。通过取这些预测的平均值，可以抵消掉一些误差。\n",
    "\n",
    "`随机森林通过组合不同的树来减少方差，有时以增加一点点偏差为代价`。在实践中，方差减少通常是值得关注的，因此产生了一个整体更好的模型。\n",
    "\n",
    "**scikit-learn实现通过平均它们的概率预测来组合分类器，而不是让每个分类器为单个类别进行投票。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_boston,load_iris\n",
    "\n",
    "li = load_iris()\n",
    "features2 = li.feature_names\n",
    "x_cls ,y_cls = li.data,li.target\n",
    "x_cls = pd.DataFrame(x_cls,columns=features2)\n",
    "y_cls = pd.DataFrame(y_cls,columns=['target'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_cls_train,X_cls_test,y_cls_train,y_cls_test = train_test_split(x_cls,y_cls,random_state=2022,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.01675679, 0.98324321],\n",
       "       [0.        , 0.01264354, 0.98735646],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.02520051, 0.97479949],\n",
       "       [1.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 默认bootstrap=True,有放回抽样，使用部分数据集\n",
    "rf_cls = RandomForestClassifier(n_estimators=100,criterion='gini',max_depth=3,max_samples=0.8,bootstrap=True,random_state=2022)\n",
    "rf_cls.fit(X_cls_train,y_cls_train)\n",
    "rf_cls.score(X_cls_train,y_cls_train)\n",
    "\n",
    "print(accuracy_score(y_cls_test,rf_cls.predict(X_cls_test)))\n",
    "rf_cls.predict_proba(X_cls_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "\n",
    "控制树大小的参数的默认值(例如max_depth, min_samples_leaf等)会导致完全生长和未修剪的树，在某些数据集上可能树会非常大。为了减少内存消耗，应通过设置这些参数值来`控制树的复杂性和大小`。\n",
    "\n",
    "在每次分割时，特性总是随机地进行分配。因此，即使训练数据相同，max_features=n_features, bootstrap=False，在搜索最佳分割的过程中，如果枚举的几个分割对准则的改进相同，那么找到的最佳分割也会有所不同。为了在拟合过程中获得确定性行为，必须确定random_state。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 极端随机树 -- ExtraTreesClassifier\n",
    "\n",
    "在极端随机树(参见 ExtraTreesClassifier 和 ExtraTreesRegressor 类)中，`计算分割点的方法随机性进一步增强`。\n",
    "\n",
    "与随机森林中一样，使用了候选特征的随机子集，但不像随机森林中是寻找最具区分度的阈值，而是`对每个候选特征随机绘制阈值，并选择这些随机生成的阈值中最佳的作为分割规则`。这种做法通常能够减少一点模型的方差，代价则是略微地增大偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0052545 , 0.24071436, 0.75403113],\n",
       "       [0.        , 0.10228781, 0.89771219],\n",
       "       [0.93719823, 0.05370412, 0.00909765],\n",
       "       [0.01406477, 0.33230632, 0.65362891],\n",
       "       [0.95267728, 0.03891332, 0.0084094 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# 默认bootstrap = False，用整个数据集。\n",
    "# 毕竟极端随机树，特征分裂阈值是随机生成的，随机性毕竟大了，所以没必要样本随机取了。\n",
    "ef_cls = ExtraTreesClassifier(n_estimators=100,criterion='gini',max_depth=3,bootstrap=False,random_state=2022)\n",
    "ef_cls.fit(X_cls_train,y_cls_train)\n",
    "ef_cls.score(X_cls_train,y_cls_train)\n",
    "\n",
    "print(accuracy_score(y_cls_test,ef_cls.predict(X_cls_test)))\n",
    "ef_cls.predict_proba(X_cls_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "使用这些方法时要调整的参数主要是 n_estimators 和 max_features。\n",
    "\n",
    "**n_estimators**是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。\n",
    "\n",
    "**max_features**是分割节点时考虑的特征的随机子集的大小。`这个值越低，方差减小得越多，但是偏差的增大也越多`。根据经验:\n",
    "\n",
    "- **回归问题**使用 `max_features = None` （考虑所有特征）；max_depth = None 和 min_samples_split = 2结合通常会有不错的效果（即完全生长的树）；\n",
    "- **分类问题**使用 `max_features = 'sqrt'` （随机考虑 sqrt(n_features) 特征）是比较好的默认值；\n",
    "\n",
    "以上这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由`交叉验证`获得。 \n",
    "\n",
    "\n",
    "另外，在随机森林中，默认使用自助采样法（bootstrap = True）， 然而 extra-trees 的默认策略是使用整个数据集（bootstrap = False）。 当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外(out-of-bag) 的样本来评估的，设置 oob_score = True 即可实现。\n",
    "\n",
    "\n",
    "注意：默认参数下模型复杂度是：O(M*N*log(N)) ， 其中 M 是树的数目， N 是样本数。 可以通过设置以下参数来`降低模型复杂度`： \n",
    "- `min_samples_split`（分割内部节点的最小样本数） ；\n",
    "- `max_leaf_nodes`（最大叶子节点的数量） ；\n",
    "- `max_depth`（最大深度）  ；\n",
    "- `min_samples_leaf`（一个叶子节点所需的最小样本数）；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要性评估\n",
    "\n",
    "特征对目标变量预测的相对重要性可通过`特征使用的相对顺序（即深度）`进行评估。在scikit-learn中，一个特征所贡献的`样本比例`与`拆分后的不纯度的减少`相结合，形成了对该特征预测能力的标准化估计。\n",
    "\n",
    "通过对几个随机树的预测能力估计值进行平均，可减少估计值的方差，并将其用于特征选择。这就是平均减少不纯度（MDI）。\n",
    "\n",
    "`基于不存度减少`计算的特征重要性有2个缺陷，\n",
    "1. 不纯度是根据训练集计算出来的，不一定在测试集上也是一样效果；\n",
    "2. 有利于高基数特征（高基数特征high cardinality features，也就是通常是数值特征，不纯度减少得更多）；\n",
    "\n",
    "基于树的模型提供一种`基于平均不纯度下降（MDI）`的特征重要性的替代测量。`不纯度`是通过决策树的分割标准（`Gini`、`Log Loss`或`Mean Squared Error`）来量化的。然而，`当模型过拟合时，这种方法会给那些可能对未见过的数据、没有预测作用的特征以很高的重要性。`所以，如果RF过拟合了，其特征重要性中，即便是对目标变量没有预测作用的特征，其重要性也会很高！！！\n",
    "\n",
    "另一方面，基于交换的特征重要性避免了这个问题，因为它可以在未见过的数据上进行计算。一般用`Permutation Importance`（排列重要性）替代。\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance\n",
    "\n",
    "Permutation Importance对于非线性或不可解释的估计器比较有用 ，基于排序的特征重要性定义为：**单个特征取值被随机打乱时模型得分的降低程度**。此过程破坏了特征与目标之间的关系，因此`模型得分的下降程度代表了模型对特征的依赖程度`。这种技术的好处在于它不依赖于模型，并且可以通过特征的不同排列进行多次计算。\n",
    "\n",
    "![20220713095408](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220713095408.png)\n",
    "\n",
    "对于一个坏的模型来说，被认为是低重要性的特征（交叉验证得分低），对于一个好的模型来说可能是非常重要的。因此，在计算重要性之前，使用验证集来评估一个模型的预测能力总是很重要的。`排序重要性并不反映一个特征本身的内在预测价值，而是反映这个特征对一个特定模型的重要程度`。(先保证模型的cv分数靠谱，再去看特征重要性才比较有意义~~)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02888889, 0.00222222, 0.13444444, 0.40777778])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 一般先训练好模型，在验证集上进行 permutation_importance 计算比较合理\n",
    "# n_repeats: 对某一特征进行排列组合的次数。\n",
    "r = permutation_importance(rf_cls,X_cls_test,y_cls_test,n_repeats=30,random_state=2022)\n",
    "r.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "排序重要性可以在训练集上计算，也可以在测试或验证集上计算。`使用测试集可以突出哪些特征对被检测模型的概括能力贡献最大`。在训练集上很重要但在测试集上不重要的特征可能会导致模型过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['neg_log_loss', 'f1_macro'] was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-899f9b88e406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'neg_log_loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'f1_macro'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpermutation_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_cls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_cls_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_cls_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_repeats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2022\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\lib\\site-packages\\sklearn\\inspection\\_permutation_importance.py\u001b[0m in \u001b[0;36mpermutation_importance\u001b[1;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mrandom_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0mbaseline_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 % estimator)\n\u001b[0;32m    429\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         raise ValueError(\"For evaluating multiple scores, use \"\n\u001b[0m\u001b[0;32m    431\u001b[0m                          \u001b[1;34m\"sklearn.model_selection.cross_validate instead. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m                          \"{0} was passed.\".format(scoring))\n",
      "\u001b[1;31mValueError\u001b[0m: For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['neg_log_loss', 'f1_macro'] was passed."
     ]
    }
   ],
   "source": [
    "# 多指标分析 , 对于不同的指标，即使重要性值的尺度有很大的不同，特征的排名也大致相同(但需要注意的是，样本不均衡的情况下，选取合适的指标)\n",
    "scoring = ['neg_log_loss','f1_macro']\n",
    "\n",
    "r2 = permutation_importance(rf_cls,X_cls_test,y_cls_test,n_repeats=30,random_state=2022,scoring=scoring)\n",
    "\n",
    "for score in r2:\n",
    "    r = r2[score]\n",
    "    print(score,r.importances_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当两个特征相关联并且其中一个特征被随机重排时，模型仍然可以通过其相关特征来访问此特征。这将导致两个特征的重要性指标降低，而这两个特征实际上可能很重要。\n",
    "\n",
    "处理此问题的一种方法是`将关联的特征聚类，并且对于每个聚类仅保留一个特征`\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完全随机树嵌入 -- RandomTreesEmbedding\n",
    "\n",
    "RandomTreesEmbedding 实现了一个`无监督数据转换`。 \n",
    "\n",
    "通过由完全随机树构成的森林，`RandomTreesEmbedding 使用数据最终归属的叶子节点的索引值对数据进行编码`。 该索引以 one-of-K 方式编码，最终形成一个高维的稀疏二值化编码。 这种编码可以被非常高效地计算出来，并且可以作为其他学习任务的基础。 \n",
    "\n",
    "`编码的大小`和`稀疏度`可以通过`选择树的数量和每棵树的最大深度来确定`。对于集成中的每棵树的，每个样本对应其中的一个叶节点。 编码的大小（维度）最多为 n_estimators * 2 ** max_depth ，即森林中的叶子节点的最大数。\n",
    "\n",
    "由于相邻数据点更可能位于一颗树的同一叶子中，该变换可以作为一种隐式地非参数密度估计。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 2 1]\n",
      " [2 1 1 2 1]\n",
      " [2 2 2 2 2]\n",
      " [1 1 1 1 1]\n",
      " [2 1 1 2 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "\n",
    "X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n",
    "\n",
    "# 5颗树构成的森林，且每颗树最大深度为1 \n",
    "random_trees = RandomTreesEmbedding(n_estimators=5, random_state=0, max_depth=1).fit(X)\n",
    "\n",
    "# 对于X中的每个数据点x和森林中的每个树，返回x最后所在叶子的索引\n",
    "print(random_trees.apply(X))\n",
    "\n",
    "# 转换数据：其实就是对叶子索引进行one-hot encode\n",
    "X_sparse_embedding = random_trees.transform(X)\n",
    "X_sparse_embedding.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先apply拿到leaf indices，再transform得到一个one-hot encode：\n",
    "\n",
    "![20220713132058](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220713132058.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boost -- AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost的核心原理：\n",
    "\n",
    "在反复修改的数据版本上拟合一连串的弱学习者（即只比随机猜测稍好的模型，如小型决策树），然后通过加权多数票将所有模型的预测结果结合起来，产生最终的预测结果。\n",
    "\n",
    "对于每个提升迭代中的数据修改，最初样本权重都被设置为wi=1/N，因此第一步只是在原始数据上训练一个弱学习器。对于每个连续迭代，样本的权重被单独修改，学习算法被重新应用于重新加权的数据。（`预测错误的样本有更高的权重`，以便下一轮学习的时候，模型更关注这些没学好的样本）\n",
    "\n",
    "在一个给定的步骤中，在上一个步骤中被提升模型错误预测的训练例子的权重被增加，而那些被正确预测的例子的权重被降低。随着迭代的进行，难以预测的例子受到越来越大的影响。因此，`每一个后续的弱学习者都专注于序列中被前面的学习器遗漏的例子，并且学习器的预测误差越小，最后学习器的权重越高`。\n",
    "\n",
    "![20220714104702](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220714104702.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# lr : 在每个提升迭代中应用于每个分类器的权重。\n",
    "# 较高的lr会增加每个分类器的贡献。在lr和n_estimators参数之间有一个权衡,n_estimators越大，lr相应的也要调大\n",
    "ab_cls = AdaBoostClassifier(n_estimators=1000,learning_rate=0.5,random_state=2022,algorithm='SAMME')\n",
    "ab_cls.fit(X_cls_train,y_cls_train)\n",
    "print(ab_cls.score(X_cls_train,y_cls_train))\n",
    "\n",
    "print(accuracy_score(y_cls_test,ab_cls.predict(X_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度提升树 -- Gradient Tree Boosting\n",
    "\n",
    "向前分阶段的方式建立了一个`加法模型`；它允许对任意可分的损失函数进行优化。\n",
    "\n",
    "在每个阶段，`回归树被拟合在损失函数的负梯度上`，例如二元或多类对数损失。\n",
    "\n",
    "![20220714105250](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220714105250.png)\n",
    "\n",
    "**GradientBoostingClassifier只适合小样本数据**，当`样本数量大于数万个`样本时，`基于直方图的估计器-HistGradientBoostingRegressor`可以比GradientBoostingClassifier快几个数量级，内置了`对缺失值的支持`，从而避免了对imputer的需求。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9666666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01}\n",
    "\n",
    "gbdt_cls = GradientBoostingClassifier(**params)\n",
    "gbdt_cls.fit(X_cls_train,y_cls_train)\n",
    "print(gbdt_cls.score(X_cls_train,y_cls_train))\n",
    "\n",
    "print(accuracy_score(y_cls_test,gbdt_cls.predict(X_cls_test)))\n",
    "\n",
    "# 计算测试集偏差\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(gbdt_cls.staged_predict(X_cls_test)):\n",
    "    test_score[i] = 1-accuracy_score(y_cls_test, y_pred) \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, gbdt_cls.train_score_, 'b-',\n",
    "         label='Training Set Deviance') # 直接拿到训练集偏差\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', \n",
    "         label='Test Set Deviance') \n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 特征重要性\n",
    "\n",
    "# 基于不纯度特征重要性\n",
    "feature_importance = gbdt_cls.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(X_cls_train.columns)[sorted_idx])\n",
    "plt.title('Feature Importance (MDI)')\n",
    "\n",
    "# 基于 permutation_importance 排序特征重要性\n",
    "result = permutation_importance(gbdt_cls, X_cls_test, y_cls_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "            vert=False, labels=np.array(X_cls_train.columns)[sorted_idx])\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8965\n"
     ]
    }
   ],
   "source": [
    "# 基于直方图改进的GBDT,适合数据量较大的数据集\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# max_iter 就是迭代次数，和gbdt里的n_estimators一样\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数选择：\n",
    "\n",
    "- 回归损失有‘least_squares’, ‘least_absolute_deviation’(对异常值不太敏感)和 ‘poisson’(非常适合模型计数和频率)。\n",
    "- 分类方面，‘binary_crossentropy’ 用于二分类， ‘categorical_crossentropy’用于多分类。默认情况下，损失函数是 ‘auto’\n",
    "\n",
    "\n",
    "如果样本数大于10,000，则默认启用早停。早期停止行为通过 `early-stopping`, validation_fraction, n_iter_no_change, `tol`参数来控制.\n",
    "\n",
    "处理缺失值：\n",
    "\n",
    "- 在训练过程中，`树根据潜在的收益，在每个分割点学习有缺失值的样本应该归入左边还是右边的子代`；当预测时，有缺失值的样本会被分配到左边或右边的子代\n",
    "\n",
    "- 当缺失模式具有预测性时，可以根据特征值是否缺失进行分割；如果在训练期间给定特征下没有缺失值，则会将有缺少值的样本映射到具有最多样本的那个子节点上；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# 看看把 nan 放哪边的收益最大（平方误差最小）\n",
    "X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\n",
    "gbdt.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 缺失模式具有预测性。X的缺失值都对应y=1 ，把有缺失值的放一边，没缺失的放一边\n",
    "X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 1, 0, 0, 1]\n",
    "\n",
    "gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\n",
    "                                      max_depth=2,\n",
    "                                      learning_rate=1,\n",
    "                                      max_iter=1).fit(X, y)\n",
    "gbdt.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "较小的learning_rate值要求较多的弱学习器以保持一个恒定的训练误差。经验证据表明，`较小的learning_rate有利于更好的测试误差`。建议`将学习速率设置为一个小常数(例如，learning_rate <= 0.1)，并通过早期的停止选择n_estimators`。\n",
    "\n",
    "降低偏差：\n",
    "\n",
    "![20220713174748](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220713174748.png)\n",
    "\n",
    "- 指定收缩率比没有收缩拥有更好的表现。\n",
    "- 将子采样和收缩率相结合能进一步的提高模型的准确率。\n",
    "- 相反，使用子采样而不使用收缩的结果十分糟糕。\n",
    "\n",
    "结论：指定学习率比没指定效果好；如果对样本子采样，就需要指定学习率。\n",
    "\n",
    "降低方差：是对分类器中类似随机分裂的`特征进行二次采样`，通过`max_features`参数可以控制子采样特征的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`单个决策树`本质上通过选择合适的分割点来进行特征选择，这些信息可以用来度量每个特征的重要性。基本想法：`一个特征在树的分割点中使用的频率越高，该特征就越重要`。这种重要性的概念可以扩展到决策树群，只需`对每棵树的基于不纯度的特征重要性进行平均`即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10256743, 0.10566776, 0.11311001, 0.09732795, 0.09885413,\n",
       "       0.1065393 , 0.08972368, 0.09697381, 0.10107558, 0.08816034])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=3, random_state=0).fit(X, y)\n",
    "\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 投票分类器 -- Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同的机器学习分类器，并使用多数票或平均预测概率（软投票）来预测类标签。这样的分类器对于一组表现同样好的模型来说是很有用的，`以平衡它们各自的弱点`。\n",
    "\n",
    "voting类型：\n",
    "- hard，使用预测的类标签进行多数规则投票\n",
    "- soft，根据预测概率之和的argmax(返回最大值索引)来预测类标签(推荐)。\n",
    "\n",
    "  - 分类器权重一样的时候，所有分类器概率均值最大的类别：\n",
    "\n",
    "    ![20220714141030](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220714141030.png)\n",
    "\n",
    "weights：\n",
    "- 对预测的类标签（硬投票）或类概率的出现进行加权。如果没有，则使用统一权重uniform。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1, 2, 2, 0, 1, 2, 2, 1, 0, 0,\n",
       "       2, 0, 2, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "cls1 = LogisticRegression(random_state=2022)\n",
    "cls2 = DecisionTreeClassifier(random_state=2022)\n",
    "cls3 = GradientBoostingClassifier(n_estimators=50,random_state=2022)\n",
    "\n",
    "estimators = [('lr',cls1),('dt',cls2),('gbdt',cls3)]\n",
    "\n",
    "ecls = VotingClassifier(estimators=estimators,voting='soft',weights=[2,1,1])\n",
    "\n",
    "ecls.fit(X_cls_train,y_cls_train)\n",
    "ecls.score(X_cls_train,y_cls_train)\n",
    "ecls.predict(X_cls_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=100, random_state=2022)),\n",
       "                             ('dt', DecisionTreeClassifier(max_depth=3)),\n",
       "                             ('gbdt',\n",
       "                              GradientBoostingClassifier(n_estimators=50,\n",
       "                                                         random_state=2022))],\n",
       "                 voting='soft', weights=[2, 1, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'lr__C':[1,100],'dt__max_depth':[3,4],'gbdt__learning_rate':[0.1,0,2,0.3]}\n",
    "\n",
    "grid = GridSearchCV(estimator = ecls,param_grid=params,cv=5)\n",
    "grid.fit(X_cls_train,y_cls_train)\n",
    "grid.best_score_\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类器堆叠 -- Stacked generalization\n",
    "\n",
    "每个单独的估计器的预测被堆叠在一起，并作为最终估计器的输入来计算预测值。这个最终估计器是通过交叉验证训练。\n",
    "\n",
    "`不同基模型代表同一问题的不同模型的空间`。使用不同类型的模型来处理机器学习问题，这些模型能够在一定程度上学习不同的东西，而不是学习问题的整个空间。并利用这些基模型可以进行中间预测，然后增加一个新的模型，可以利用中间预测进行学习，可以提高模型性能，并且总是可以得到一个比中间模型更好的模型。\n",
    "\n",
    "`estimators_`是在完整的X上拟合的，而`final_estimator_是使用cross_val_predict对基础估计器的交叉验证预测进行训练`。\n",
    "\n",
    "![20220714180112](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220714180112.png)\n",
    "\n",
    "- https://developer.ibm.com/articles/stack-machine-learning-models-get-better-results/\n",
    "- http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 学习器与其参数\n",
    "estimators = [('ridge', RidgeCV()),\n",
    "              ('lasso', LassoCV(random_state=42)),\n",
    "              ('knr', KNeighborsRegressor(n_neighbors=20,\n",
    "                                          metric='euclidean'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_estimator将使用估计器的预测结果作为输入:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5013683662803119"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "final_estimator = LinearRegression()\n",
    "\n",
    "reg = StackingRegressor(estimators=estimators,final_estimator=final_estimator)\n",
    "\n",
    "####训练各个学习器与最后的学习器####\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "li = load_diabetes()\n",
    "features2 = li.feature_names\n",
    "X ,y = li.data,li.target\n",
    "X = pd.DataFrame(X ,columns=features2)\n",
    "y = pd.DataFrame(y,columns=['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)\n",
    "\n",
    "reg.fit(X_train, y_train).score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                 1.000000\n",
      "stacking_prediction    0.708465\n",
      "ridge                  0.701646\n",
      "lasso                  0.699865\n",
      "knr                    0.699294\n",
      "Name: target, dtype: float64\n",
      "('ridge', 1.2331910294273383)\n",
      "('lasso', -0.3212077196097366)\n",
      "('knr', 0.20356728812193053)\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame(y_test)\n",
    "\n",
    "# stacked models 预测\n",
    "temp['stacking_prediction'] = reg.predict(X_test)\n",
    "# 每个estimator的预测\n",
    "for m in reg.named_estimators_:\n",
    "        temp[m] = reg.named_estimators_[m].predict(X_test)\n",
    "        \n",
    "# 不同模型预测结果的相关性\n",
    "print(temp.corr()['target'])\n",
    "\n",
    "# 基模型如何线性组合成 final_estimator\n",
    "for coef in zip(reg.named_estimators_, reg.final_estimator_.coef_):\n",
    "    print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证防止过拟合(借助mlxtend包)：\n",
    "\n",
    "将数据集分成K份，基模型在每一轮训练时，用k-1份数据进行fit，剩下的1份进行predict，直至迭代训练、预测完成，生成和原始数据维度一样的预测数据，输入stacked model进行训练并预测."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "\n",
    "def dataset_to_df(load):\n",
    "    df = pd.DataFrame(load.data, columns=load.feature_names)\n",
    "    df['label'] = pd.Series(load.target)\n",
    "    return df\n",
    "\n",
    "df = dataset_to_df(load_boston())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['label']\n",
    "X = df.drop(['label'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X, y,\n",
    "                                    test_size=0.33,\n",
    "                                    random_state=42\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "lgbm = LGBMRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "stack = StackingCVRegressor(regressors=(ridge, lasso, svr, rf, lgbm, xgb),\n",
    "                            meta_regressor=xgb, cv=12,\n",
    "                            use_features_in_secondary=True,\n",
    "                            store_train_meta_features=True,\n",
    "                            shuffle=False,\n",
    "                            random_state=42)\n",
    "\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "X_test.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12']\n",
    "pred = stack.predict(X_test)\n",
    "score = r2_score(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
